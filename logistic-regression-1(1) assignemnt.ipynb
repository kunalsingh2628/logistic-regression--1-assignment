{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c290f2d-515c-40b1-b6f9-953cebaf0579",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a459451-25c8-4a15-96ca-be8a186a9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression is used for predicting a continuous outcome variable based on one or more predictor variables. It establishes a linear relationship between the dependent and independent variables. The output is a real-valued number.\n",
    "\n",
    "Logistic regression, on the other hand, is used for binary classification problems. It predicts the probability of an instance belonging to a particular category. The logistic function (sigmoid function) is applied to the linear combination of input features, which transforms the output into a probability between 0 and 1.\n",
    "\n",
    "Example scenario:\n",
    "\n",
    "Linear Regression: Predicting house prices based on features like square footage, number of bedrooms, etc.\n",
    "Logistic Regression: Predicting whether an email is spam (1) or not spam (0) based on features like the presence of certain keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7e61a-e7c1-458b-8afc-3f6def2271d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b289e4f-f8bc-4b4a-8646-0493a3296c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The cost function used in logistic regression is the cross-entropy loss (also known as log loss). For a binary classification problem, it is defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "[\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    ")\n",
    "]\n",
    "J(θ)=− \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " [y \n",
    "(i)\n",
    " log(h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))+(1−y \n",
    "(i)\n",
    " )log(1−h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))]\n",
    "\n",
    "where \n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "h \n",
    "θ\n",
    "​\n",
    " (x) is the sigmoid function, and \n",
    "�\n",
    "y is the actual class label.\n",
    "\n",
    "The optimization is typically done using iterative optimization algorithms like gradient descent. The goal is to minimize the cost function by adjusting the parameters (\n",
    "�\n",
    "θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f00c6-4840-4494-bbd3-3dd54d86bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff0ff2-a7cb-4c25-920d-f2c91e117647",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization in logistic regression involves adding a penalty term to the cost function to prevent overfitting. The two common types of regularization are L1 regularization (Lasso) and L2 regularization (Ridge). The regularization term is added to the cost function, influencing the learning algorithm to keep the model parameters smaller.\n",
    "\n",
    "The regularized cost function is:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "[\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "�\n",
    "(\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    ")\n",
    "]\n",
    "+\n",
    "�\n",
    "2\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "J(θ)=− \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " [y \n",
    "(i)\n",
    " log(h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))+(1−y \n",
    "(i)\n",
    " )log(1−h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))]+ \n",
    "2m\n",
    "λ\n",
    "​\n",
    " ∑ \n",
    "j=1\n",
    "n\n",
    "​\n",
    " θ \n",
    "j\n",
    "2\n",
    "​\n",
    " \n",
    "\n",
    "where \n",
    "�\n",
    "λ is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2ce69-e2f4-4932-8792-3c41878a645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9f8e3-90dd-4ff5-9153-005769ae8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) at various thresholds. It helps evaluate the performance of a logistic regression model in differentiating between the two classes.\n",
    "\n",
    "A perfect model would have an area under the ROC curve (AUC) equal to 1, while a random model would have an AUC of 0.5. The higher the AUC, the better the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf6866-0913-4839-adf3-a2c1fa837f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0983db9-fb0d-4270-b291-a6e8d1b719f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Common techniques for feature selection include:\n",
    "\n",
    "Recursive Feature Elimination (RFE): Iteratively removes the least significant features and trains the model until the optimal number of features is achieved.\n",
    "\n",
    "L1 regularization (Lasso): The regularization term encourages sparsity in the model parameters, effectively performing feature selection.\n",
    "\n",
    "Feature Importance from Trees: For models like Random Forest or Gradient Boosted Trees, features can be ranked by their importance.\n",
    "\n",
    "These techniques help improve model performance by reducing overfitting, enhancing interpretability, and potentially speeding up training and inference. Removing irrelevant or redundant features can also simplify the model without sacrificing accurac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5d087-66e9-4d39-8912-c856a5856dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc6a6d-e585-4bda-9075-314188fa85f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db505060-cd0f-48c2-9673-cde2f93b38d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbe75b-c0a1-4707-a108-e8e4f6aa6a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690f1b7-405a-4f12-9b49-2a6320507aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab31cb-dbd6-462e-9334-f6ac017993df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
